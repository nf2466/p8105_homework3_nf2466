---
title: "Homework 2"
author: Nancy Fang (nf2466)
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(hexbin)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```

There are 134 aisles and the most items are from the fresh vegetables and the fresh fruits aisle.

Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

This table shows the top three product counts in baking ingredients, dog food care, and packaged vegetables/fruits.

Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>%
  mutate(order_dow=recode(order_dow,
       "0"="Sunday",
       "1"="Monday",
       "2"="Tuesday",
       "3"="Wednesday",
       "4"="Thursday",
       "5"="Friday",
       "6"="Saturday"),
       order_dow=as.factor(order_dow))%>%
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)%>%
  knitr::kable()
```

This table shows people tend to buy Pink Lady Apples a little earlier in the day compared to coffee ice cream, although both seem be bought in the early to late afternoon.

### Problem 2

_Part a: Load the dataset and tidy_
```{r}
accel_df=
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names()%>%
  pivot_longer(activity_1:activity_1440, names_to = "activity_minute", names_prefix = "activity_", values_to = "activity_count")%>%
  mutate(activity_minute = as.numeric(activity_minute),
         day_id=as.factor(day_id),
         day=as.factor(day),
  day = factor(day, levels = str_c(c("Mon", "Tues", "Wednes", "Thurs", "Fri", "Satur", "Sun"),"day"))
  )%>%
  arrange(week, day)%>%
  mutate(
  day_type=recode(day, "Saturday" = "weekend", "Sunday" = "weekend", 
                  "Monday" = "weekday", "Tuesday" = "weekday", "Wednesday" = "weekday", 
                  "Thursday" = "weekday","Friday" = "weekday")
  )
   
```
_Part b: Now, aggregate total day activity count._

```{r}

accel_df%>%
  group_by(week,day)%>%
  summarize(count_total=sum(activity_count))%>%
  pivot_wider(
    names_from= day,
    values_from = count_total
  )%>%
  knitr::kable()
```
There doesn't appear to be a strong pattern in terms of the total activity count over the 5 weeks. Overall, the activity total appears highest on Mondays and lowest on Saturdays.

_Part c:_
```{r}
 accel_df_p =
  accel_df %>%
  ggplot(aes(x = activity_minute, y = activity_count, color = day)) + 
   geom_smooth(se=FALSE) +
  labs(
    title = "Daily activity by day",
    x = "Minutes in a day (24 hours)",
    y = "Activity count"
  )

accel_df_p

```

It appears that there is an expected pattern of low activity during sleeping hours and more activity during the middle of the day (between minute 500-1000). We also see that on average, there is more activity in the morning/afternoon on Sunday and more activity in the evening time on Fridays.

### Problem 3

_Part a: Download the NOAA dataset from P8105_
```{r}
library(p8105.datasets)
data("ny_noaa")

```
In this dataset, there are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. There are `r ny_noaa%>%distinct(id)%>%count()` distinct locations in New York state. The variables also include the date of data collection, between `r min(ny_noaa$date)` and `r max(ny_noaa$date)`, precipitation, snowfall, snow depth, max temp and min temp in Celsius. There are `r sum(is.na(ny_noaa$prcp))` missing values for precipitation, `r sum(is.na(ny_noaa$snow))` missing values for snowfall, `r sum(is.na(ny_noaa$snwd))` missing values for snow depth, `r sum(is.na(ny_noaa$tmax))` missing values for max temp and `r sum(is.na(ny_noaa$tmin))` missing values for min temp.

_Data cleaning:_
```{r}
noaa_df=
  ny_noaa%>%
  janitor::clean_names()%>%
  separate(date, into = c('year','month','day')) %>%
  mutate(month = month.abb[as.factor(month)]) %>%
  mutate_at(vars(year, day, tmax, tmin), as.numeric)%>%
  mutate(tmax=tmax/10, tmin=tmin/10)
  
            
```
For snowfall, what are the most commonly observed values? Why?
```{r}
noaa_df%>%
  drop_na(snow)%>%
  group_by(snow)%>%
  summarize(snow_n=n())%>%
  arrange(desc(snow_n))
```
The most commonly observed value is 0. This is likely because it does not snow on most days of the year in New York.

_Part b: Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?_
```{r}
noaa_avgtmax_p =
  noaa_df%>%
  drop_na(tmax)%>%
  filter(month==c("Jan","Jul"))%>%
  group_by(id,year,month)%>%
  summarize(avg_tmax=mean(tmax))%>%
  ggplot(aes(x = year, y = avg_tmax, group = id, color = id)) +
  geom_point() +
  geom_path() +
  facet_grid(~month)+
  guides(color="none") +
  labs(
    title = "Average max temperature in January and July by ID",
    x = "Year",
    y = "Average max temperature (Celsius)"
  )

noaa_avgtmax_p
```

In general, we see that the average max temperature in Celsius is lower in January than in July. During years where the average max temperature was lower, all data points are lower. There does not seem to be a trend of increasing average max temperatures from these graphs. There are very few outliers in our data set.

_Part c: Make a two-panel plot showing (i) tmax vs tmin for the full dataset; and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year._

```{r}
 maxvmin_p = 
  noaa_df%>%
  drop_na(tmax,tmin)%>%
  group_by(id)%>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex(bins = 30) +
  guides(color="none") +
  theme(legend.position = "right", legend.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  labs(
    caption = "Min temperature vs max temperature",
    x = "Minimum temperature (Celsius)",
    y = "Maximum temperature (Celsius)",
    size=rel(0.50)
  )

snowfall_p =
  noaa_df %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x =snow, y=year, group = year)) +
  ggridges::geom_density_ridges(scale = 0.85) + 
  labs(
    caption = "The Distribution of Snowfall",
    y = "Year",
    x = "Snowfall in mm",
    size=rel(0.50)
    )

maxvmin_p + snowfall_p
```

The Tmin vs Tmax plot shows us that most places had a minimum temperature around 15 degrees Celsius and a maximum temperature around 25 degrees Celsisus. The distribution of snowfall graph shows that days there was a total snowfall of between 0-40 mm but there were many days where snowfall was ~60 mm or even as high as ~75 mm.
